# generated by GPT-4

import requests
from bs4 import BeautifulSoup, NavigableString
import re
from tqdm.notebook import tqdm
import pandas as pd
import os
import yaml
import json
from sklearn.model_selection import train_test_split
import numpy as np
import textwrap

def download_website_content(url):
    try:
        response = requests.get(url)
        response.raise_for_status()  # Raises an HTTPError if the response status code is 4XX/5XX
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')
        # text markup for which a space needs to be prepended and appended
        tags = ['a', 'span', 'li', 'strong', 'br', 'b', 'i', 'em', 'mark', 'small', 'del', 'ins', 'sub', 'sup']
        for element in soup.find_all(tags):
            element.insert_before(NavigableString(' '))
            element.insert_after(NavigableString(' '))

        return soup.get_text()
    except requests.RequestException as e:
        print(f"Failed to download {url}: {e}")
        return None


def save_content_to_file(content, path):
    # remove redundant whitespace
    content = '\n'.join(line for line in content.splitlines() if line.strip())
    while "  " in content:
        content = content.replace("  ", " ")
    content = '\n'.join(line.strip() for line in content.splitlines())

    with open(path, 'w', encoding='utf-8') as file:
        file.write(content)


def download_input_data(input_file, output_dir, overwrite=False):
    os.makedirs(output_dir, exist_ok=True)
    df = pd.read_csv(input_file)

    # Group by 'journal_abbr' and 'website'
    grouped = df.groupby(['journal_abbr', 'website'])

    # Initialize tqdm progress bar
    pbar = tqdm(total=len(grouped), desc="Downloading Content")

    downloaded_pages = 0

    for (journal_abbr, url), _ in grouped:

        filename = f"{output_dir}/{re.sub(r'[. ]', '', journal_abbr)}.txt"

        # no need to download if we don't have a URL or the website has already been downloaded
        if pd.isna(url) or (os.path.exists(filename) and not overwrite):
            pbar.update(1)
            continue

        content = download_website_content(url)
        if content:
            save_content_to_file(content, filename)
            downloaded_pages += 1
        else:
            print(f"Failed to retrieve content for {journal_abbr} - {url}")
        pbar.update(1)

    pbar.close()

    print(f'Downloaded {downloaded_pages} web pages.')


def wrap_content_generator(content, width=80):
    for line in content.splitlines(keepends=True):
        if line.strip() == '':
            yield '\n'
        else:
            for line2 in textwrap.wrap(line, width=width):
                yield line2

def clean_values(row):
    """Return a new dictionary with no NaN, None, or empty/whitespace-only values."""
    return {k: v for k, v in row.items() if pd.notna(v) and v is not None and v.strip()}


def create_training_file(instruction, input_file, output_dir, website_dir,
                         cols_to_remove: list, column_to_filter_by: str,
                         line_width=120, lines_before=2, lines_after=2, random_seed=None,
                         debug=True):
    # Load the input
    df = pd.read_csv(input_file)

    # Set or generate a random seed
    if random_seed is None:
        random_seed = np.random.randint(0, 10000)

    # Prepare the output directory
    os.makedirs(output_dir, exist_ok=True)

    # Split the DataFrame into training (80%) and test+validation (20%)
    train_df, test_valid_df = train_test_split(df, test_size=0.2, random_state=random_seed)

    # Further split the test+validation into test and validation (50% each of the 20%)
    test_df, valid_df = train_test_split(test_valid_df, test_size=0.5, random_state=random_seed)

    def process_and_write_data(grouped_df, file, write_debug_file=False):
        for journal_abbr, group in grouped_df.groupby('journal_abbr'):
            filename = f"{website_dir}/{re.sub(r'[. ]', '', str(journal_abbr))}.txt"
            try:
                with open(filename, 'r', encoding='utf-8') as content_file:
                    website_content = content_file.read()
            except FileNotFoundError:
                continue

            # wrap content to decrease token window
            lines = [line for line in wrap_content_generator(website_content, width=line_width)]
            wrapped_content = '\n'.join(lines)

            # filter rows
            keyword_list = [x for x in group[column_to_filter_by].tolist() if pd.notnull(x)]
            filtered_content = filter_content_with_context(wrapped_content, keywords=keyword_list,
                                                          lines_before=lines_before, lines_after=lines_after)
            if filtered_content.strip() == '':
                continue

            answer_df = group.drop(cols_to_remove, axis=1, errors='ignore').dropna(axis=1, how='all')

            # Convert DataFrame rows to dictionaries, cleaning NaN values
            cleaned_rows = answer_df.apply(lambda row: clean_values(row.to_dict()), axis=1)
            answer_yaml = yaml.dump(list(cleaned_rows), allow_unicode=True, sort_keys=False)

            if write_debug_file:
                prompt = [
                    '### JOURNAL', journal_abbr,
                    '### URL', group['website'].tolist()[0],
                    '### CONTENT', filtered_content,
                    '### ANSWER', answer_yaml,
                    '-' * line_width,
                    ''
                ]
                file.write('\n\n'.join(prompt))
            else:
                prompt = [
                    '### INSTRUCTION', instruction,
                    '### CONTENT', filtered_content,
                    '### ANSWER', answer_yaml
                ]
                train_json = {
                    "id": journal_abbr,
                    "text": '\n\n'.join(prompt)
                }
                file.write(json.dumps(train_json) + '\n')

    # Open files for writing
    with open(f'{output_dir}/train.jsonl', 'w', encoding='utf-8') as train_file, \
            open(f'{output_dir}/test.jsonl', 'w', encoding='utf-8') as test_file, \
            open(f'{output_dir}/valid.jsonl', 'w', encoding='utf-8') as valid_file, \
            open(f'{output_dir}/debug.txt', 'w', encoding='utf-8') as debug_file:

        # Debug file for better readability of the result
        instruction = instruction.replace('\n', '\n\n')
        instruction = "\n".join(textwrap.wrap(instruction, width=line_width))
        debug_file.write(f'### INSTRUCTION\n\n{instruction}\n\n{"=" * line_width}\n\n')
        if debug:
            process_and_write_data(df, debug_file, write_debug_file=True)

        # Process and write data to each file
        process_and_write_data(train_df, train_file)
        process_and_write_data(test_df, test_file)
        process_and_write_data(valid_df, valid_file)


def filter_content_with_context(content, keywords, lines_before=1, lines_after=1):
    """
    Filters website content to include lines containing any of the specified keywords as whole words,
    along with a specified number of lines before and after for context. This version uses regular expressions
    to ensure exact, whole word matching and respects case sensitivity.

    Parameters:
    - content: The full text content of the website.
    - keywords: An array of strings to search for within the content.
    - lines_before: Number of lines to include before a matching line.
    - lines_after: Number of lines to include after a matching line.

    Returns:
    A string containing the filtered content with additional context.
    """
    lines = content.splitlines()
    matched_lines = set()

    for i, line in enumerate(lines):
        for keyword in keywords:
            # Use a regular expression to find whole word matches with exact case
            if re.search(r'\b' + re.escape(keyword) + r'\b', line):
                context_range = range(max(i - lines_before, 0), min(i + lines_after + 1, len(lines)))
                matched_lines.update(context_range)

    # Extract the matched lines and their context
    filtered_lines = [lines[i] for i in sorted(matched_lines)]

    # Join the filtered lines back into a single string
    filtered_content = "\n".join(filtered_lines)

    return filtered_content
