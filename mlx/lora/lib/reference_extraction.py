# generated by GPT-4
import re
import pandas as pd
import os
import yaml
import json
from sklearn.model_selection import train_test_split
import numpy as np
import textwrap
from lxml import etree


def xml_to_dataframe(xml_path):
    # Parse the XML file
    tree = etree.parse(xml_path)
    root = tree.getroot()

    # Initialize an empty list to store row data
    rows = []

    # Iterate through each sequence in the XML
    for sequence in root.findall('sequence'):
        # Initialize an empty dictionary for each sequence
        row_data = {}

        # Iterate through each child element in the sequence
        for element in sequence:
            # Use tag name as column name and text as the value in the row_data dictionary
            row_data[element.tag] = element.text

        # Append the dictionary to the rows list
        rows.append(row_data)

    # Convert the list of dictionaries to a DataFrame
    df = pd.DataFrame(rows)

    # Return the DataFrame
    return df

import os

def load_xmls_from_directory(directory_path):
    # Initialize an empty DataFrame to store all data
    all_data = pd.DataFrame()

    # Iterate over every file in the directory
    for filename in os.listdir(directory_path):
        # Construct the full file path
        file_path = os.path.join(directory_path, filename)

        # Check if the file is an XML file
        if os.path.isfile(file_path) and file_path.endswith('.xml'):
            # Load the XML file into a DataFrame
            df = xml_to_dataframe(file_path)

            # Append the data to the all_data DataFrame
            all_data = pd.concat([all_data, df], ignore_index=True)

    # Return the combined DataFrame
    return all_data



def clean_values(row):
    """Return a new dictionary with no NaN, None, or empty/whitespace-only values."""
    return {k: v for k, v in row.items() if pd.notna(v) and v is not None and v.strip()}

def create_training_files(instruction: str,
                         template_func: callable,
                         input_file: str, output_dir: str, content_dir: str,
                         cols_to_remove: list, column_to_filter_by: str,
                         record_identifier_col: str,
                         max_chars=2048 * 4, max_gt_items=10,
                         line_width=120, lines_before=1, lines_after=1, random_seed=None,
                         debug=True):


    # Load the input
    df = pd.read_csv(input_file)

    # Set or generate a random seed
    if random_seed is None:
        random_seed = np.random.randint(0, 10000)

    # Prepare the output directory
    os.makedirs(output_dir, exist_ok=True)

    # Keep track of the longest prompts
    id_value_pairs = {}

    def process_and_write_data(grouped_df, file, write_debug_file=False):
        for id, group in grouped_df.groupby(record_identifier_col):

            # use only the first 10 ground truth items so that the training record does not become too large
            group = group.head(max_gt_items)

            # load website content from the cache
            filename = f"{content_dir}/{re.sub(r'[. ]', '', str(id))}.txt"
            try:
                with open(filename, 'r', encoding='utf-8') as content_file:
                    content = content_file.read()
            except FileNotFoundError:
                continue

            # Clean up data
            answer_df = group.drop(cols_to_remove, axis=1, errors='ignore').dropna(axis=1, how='all')

            # Convert DataFrame rows to YAML dictionaries, cleaning NaN values
            cleaned_rows = answer_df.apply(lambda row: clean_values(row.to_dict()), axis=1)
            answer = yaml.dump(list(cleaned_rows), allow_unicode=True, sort_keys=False)

            # wrap content to decrease token window
            lines = [line for line in wrap_content_generator(content, width=line_width)]
            wrapped_content = '\n'.join(lines)

            # Determine how many characters are available for the content
            max_chars_for_content = max_chars - len(instruction) - len(answer)

            # Determine which keywords to filter by
            keyword_list = [x for x in group[column_to_filter_by].tolist() if pd.notnull(x)]

            # Filter rows, using keywords and a context window
            filtered_content = filter_content_with_context(wrapped_content,
                                                           keywords=keyword_list,
                                                           lines_before=lines_before, lines_after=lines_after,
                                                           max_chars=max_chars_for_content)

            # Ignore if nothing was found
            if filtered_content.strip() == '':
                continue

            if write_debug_file:
                sequence = [
                    '### JOURNAL', id,
                    '### URL', group['website'].tolist()[0],
                    '### CONTENT', filtered_content,
                    '### ANSWER', answer,
                    '-' * line_width,
                    ''
                ]
                file.write('\n\n'.join(sequence))
            else:
                sequence = template_func(instruction, filtered_content, answer)
                train_json = {
                    "id": id,
                    "text": sequence
                }
                file.write(json.dumps(train_json) + '\n')
                id_value_pairs[id] = len(sequence)

    # Split the DataFrame into training (80%) and test+validation (20%)
    train_df, test_valid_df = train_test_split(df, test_size=0.2, random_state=random_seed)

    # Further split the test+validation into test and validation (50% each of the 20%)
    test_df, valid_df = train_test_split(test_valid_df, test_size=0.5, random_state=random_seed)

    # Debug file for better readability of the result
    if debug:
        debug_instruction = instruction.replace('\n', '\n\n')
        debug_instruction = "\n".join(textwrap.wrap(debug_instruction, width=line_width))
        with open(f'{output_dir}/debug.txt', 'w', encoding='utf-8') as debug_file:
            debug_file.write(f'### INSTRUCTION\n\n{debug_instruction}\n\n{"=" * line_width}\n\n')
            process_and_write_data(df, debug_file, write_debug_file=True)

    # Write train, test and validation files
    with open(f'{output_dir}/train.jsonl', 'w', encoding='utf-8') as train_file, \
            open(f'{output_dir}/test.jsonl', 'w', encoding='utf-8') as test_file, \
            open(f'{output_dir}/valid.jsonl', 'w', encoding='utf-8') as valid_file:

        # Process and write data to each file
        process_and_write_data(train_df, train_file)
        process_and_write_data(test_df, test_file)
        process_and_write_data(valid_df, valid_file)

    if debug:
        print("Length of generated sequences:")
        print(f" - max: {max(id_value_pairs.values())}")
        print(f" - avg: {sum(id_value_pairs.values()) / len(id_value_pairs)}")

        sorted_pairs = sorted(id_value_pairs.items(), key=lambda x: x[1], reverse=True)
        highest_10_values = sorted_pairs[:10]
        print("Longest sequences:")
        for _id, value in highest_10_values:
            print(f"{_id}: {value}")
