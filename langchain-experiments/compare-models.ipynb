{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Comparing OpenAI and open LLMs\n",
    "\n",
    "Using the [text-only content of the website of the journal AUR - Agrar- und Umweltrecht](data/input/journal-website.txt), \n",
    "we compare the performance of GPT-4, GPT-3.5-turbo and Models available on Huggingface.\n",
    "\n",
    "## Preparation\n",
    "\n",
    "Import dependencies, define shorthand functions, and prepare test data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "396ebd275b60c720"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import io\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def response_to_df(response):\n",
    "    data = io.StringIO(response)\n",
    "    try:\n",
    "        return pd.read_csv(data)\n",
    "    except:\n",
    "        raise RuntimeError(f\"Error while parsing response:\\n{response}\")\n",
    "\n",
    "def use_model(model, template, **params):\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = (\n",
    "            prompt\n",
    "            | model\n",
    "            | StrOutputParser()\n",
    "    )\n",
    "    return response_to_df(chain.invoke(params))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e46d0648c1c6c96a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prompt\n",
    "\n",
    "OpenAI's GPT-4 works perfectly with a minimal, German-language prompt, and infers the meaning of the columns\n",
    "to returns the data we need:\n",
    "\n",
    "```\n",
    "Finde im folgenden Text die Herausgeber, Redaktion/Schriftleitung und Beirat der Zeitschrift '{journal_name}' und gebe sie im CSV-Format zurück mit den Spalten 'lastname', 'firstname', 'title', 'position', 'affiliation','role'. Die Spalte 'role' enthält entweder 'Herausgeber', 'Redaktion', 'Beirat', 'Schriftleitung' oder ist leer wenn nicht bestimmbar. Wenn keine passenden Informationen verfügbar sind, gebe nur den CSV-Header zurück. Setze alle Werte in den CSV-Spalten in Anführungszeichen.\"\n",
    "````\n",
    "\n",
    "\n",
    "In contrast, the open models performed miserably with such a prompt. We therefore use English and provide very detailed instructions.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb805270480fbfb5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('data/input/journal-website.txt', encoding='utf-8') as f:\n",
    "    website_text = f.read()\n",
    "journal_name = \"AUR - Agrar- und Umweltrecht\"\n",
    "\n",
    "template = \"\"\"\n",
    "In the following German text, which was scraped from a website, find the members of the editorial board or the advisory board of the journal '{journal_name}' as per the following rules:\n",
    "- In German, typical labels for these roles are \"Herausgeber\", \"Redaktion/Redakteur/Schriftleitung\" and \"Beirat\".  \n",
    "- Return the data as comma-separated values, which can be saved to a `.csv` file. Put all values in the CSV rows in quotes. \n",
    "- The CSV data must have the columns 'lastname', 'firstname', 'title', 'position', 'affiliation','role'. \n",
    "- The column 'role' must contain either 'Herausgeber', 'Redaktion', 'Beirat' or is empty. Leave the column empty if you cannot determine the role. Use 'Redaktion' for the \"Schriftleitung\" role.\n",
    "- The column 'title' should contain academic titles, such as \"Dr.\" or \"Prof. Dr.\"\n",
    "- The column 'position' should contain the job title, typically \"Rechtsanwalt\", \"Regierungsrat\" or \"Richter am Oberlandesgericht\"\n",
    "- The column 'affiliation' contains the institution or organization the person belongs to, or the city if one is mentioned\n",
    "- If the journal is published (\"herausgeben von\") by an association, institute or other organization, but its name in the column 'lastname'. \n",
    "- If you cannot find any information, simply return the CSV header. \n",
    "- You must not output any introduction, commentary or explanation such as 'Here is the CSV data for the members of the editorial board or the advisory board of the journal'. Only return the data.\n",
    "\n",
    "{website_text}\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23aef80911796078"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ChatGPT-4 \n",
    "\n",
    "GPT-4 delivers an almost perfect [result](data/output/editors-openai-gpt-4.csv). There are some problems left which could be resolved by adding some more instructions to the prompt. \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f994c771cc9b4ef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model_name=\"gpt-4\")\n",
    "df = use_model(model, template, journal_name=journal_name, website_text=website_text)\n",
    "df.to_csv('data/output/editors-openai-gpt-4.csv')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ChatGPT 3.5-turbo\n",
    "\n",
    "GPT-3.5 [performs less well](data/output/editors-openai-gpt-3.5-turbo.csv), but still ok. It gets some of the 'title' amd 'position' \n",
    "column data confused, and does not recognize the institutional publisher (Herausgeber) of the journal. \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cef77bb17d57b53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "df = use_model(model, template, journal_name=journal_name, website_text=website_text)\n",
    "df.to_csv('data/output/editors-openai-gpt-3.5-turbo.csv')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1aedc5ef3cab564"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's try the open models via the Huggingface Inference Endpoint. For this to work, you need to deploy\n",
    "endpoints via https://ui.endpoints.huggingface.co/ and update the value of `enpoint_url` below."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31084716e138fe06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TheBloke/Llama-2-13B-chat-GPTQ \n",
    "\n",
    "The [LLama2 13 billion parameter model](https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ) produces [unusuable output](data/output/editors-llama-2-13b-chat-gptq.txt)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90ce37abf037e19f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from lib.hf_llama2_chat_gptq import query\n",
    "llama2_template = f\"<s>[INST] <<SYS>>You are a helpful assistant. No comments or explanation, just answer the question.<</SYS>>{template}[/INST]\"\n",
    "\n",
    "endpoint_url = \"https://z8afrqamxvaaitmf.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "query(endpoint_url, template, journal_name=journal_name, website_text=website_text).split(\"\\n\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f05098a4cf2aa3dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TheBloke/Llama-2-70B-chat-GPTQ via Huggingface Inference Endpoint\n",
    "\n",
    "The 70 billion parameter variant [does a bit better](data/output/editors-llama-2-70b-chat-gptq.csv) but, among other things, doesn't the academic titles right. It also cannot be persuaded to [not comment on the CSV output](data/output/editors-llama-2-70b-chat-gptq.txt)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca33fb28f6772cbc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "endpoint_url = \"https://gp8iviqlqee101a0.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "query(endpoint_url, template, journal_name=journal_name, website_text=website_text).split(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b94cf62b996bf3a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## mixtral-8x7b-instruct-v0-1-puk"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b9b6d23dcdbbdd6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from lib.hf_llama2_chat_gptq import query\n",
    "llama2_template = f\"<s>[INST] <<SYS>>You are a helpful assistant. You answer the question without any further No comments or explanation.<</SYS>>{template}[/INST]\"\n",
    "\n",
    "endpoint_url = \"https://pmxm9cba6f8uvi9s.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "query(endpoint_url, template, journal_name=journal_name, website_text=website_text).split(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5eb505982b3aafe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "\n",
    "ENDPOINT_URL = \"https://pmxm9cba6f8uvi9s.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "llm = HuggingFaceEndpoint(\n",
    "    endpoint_url=ENDPOINT_URL,\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 50,\n",
    "        \"temperature\": 0.1,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    },\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d5b65beac1f863e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "from langchain_community.llms import HuggingFaceTextGenInference\n",
    "\n",
    "ENDPOINT_URL = \"https://pmxm9cba6f8uvi9s.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "llm = HuggingFaceTextGenInference(\n",
    "    inference_server_url=ENDPOINT_URL,\n",
    "    max_new_tokens=512,\n",
    "    top_k=50,\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=1.03,\n",
    "    server_kwargs={\n",
    "        \"headers\": {\n",
    "            \"Authorization\": f\"Bearer {HF_TOKEN}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "    },\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58e87b84bdc2e4d3"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "\n",
    "messages = [\n",
    "    #SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(\n",
    "        content=\"What happens when an unstoppable force meets an immovable object?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T12:56:44.407240200Z",
     "start_time": "2024-02-19T12:56:42.802384100Z"
    }
   },
   "id": "411c80521b6f7ccb"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9f2e4ce2c28741e0"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is a classic philosophical question that has been asked for centuries, often used to illustrate a paradox. The idea of an \"unstoppable force\" implies something that cannot be stopped or slowed down, while an \"immovable object\" suggests something that cannot be moved. \n",
      "\n",
      "If we take these definitions literally, then when an unstoppable force meets an immovable object, it would result in a situation where neither can fulfill their inherent nature. This creates a paradox because the force cannot stop, but the object won't be moved. \n",
      "\n",
      "In reality, such a scenario is impossible as it defies the laws of physics. Forces and objects in the universe do not possess these absolute qualities. Instead, forces can typically be slowed, redirected, or absorbed, and objects can usually be moved, albeit sometimes with great difficulty.\n"
     ]
    }
   ],
   "source": [
    "res = chat_model.invoke(messages)\n",
    "print(res.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T12:56:58.327411200Z",
     "start_time": "2024-02-19T12:56:51.135872200Z"
    }
   },
   "id": "2b2cbf175a71930b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
